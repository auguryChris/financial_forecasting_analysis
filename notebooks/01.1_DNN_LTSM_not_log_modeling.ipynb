{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta, datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from scipy.interpolate import CubicSpline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, LeakyReLU\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tqdm import tqdm\n",
    "import sys, os\n",
    "\n",
    "experiment_time = datetime.now().strftime(\"%H_%M_%S_%m_%d_%Y\")\n",
    "\n",
    "\n",
    "features_in_order = ['Open', 'High', 'Low', 'Volume', 'Close'] # target feature must be the last one here\n",
    "target_feature = 'Close'\n",
    "\n",
    "absolutepath = os.path.abspath('')\n",
    "fileDirectory = os.path.dirname(absolutepath)\n",
    "\n",
    "class ManyToOneTimeSeriesGenerator(TimeseriesGenerator):\n",
    "  def __getitem__(self, idx):\n",
    "    x, y = super().__getitem__(idx)\n",
    "    last_element_index = y.shape[1]-1\n",
    "    return x, y[:,last_element_index].reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'09_16_52_11_21_2021'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.19.5'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to model later IMFs as splines\n",
    "class SplineModel():\n",
    "    def __init__(self,time_series_generator):\n",
    "        self.name = \"SplineModel\"\n",
    "        self.gen = time_series_generator\n",
    "    \n",
    "    def predict(self, x_window, verbose=0):\n",
    "        result = []\n",
    "        x_window = np.squeeze(x_window, axis=0)\n",
    "        last_element_index = x_window.shape[1]-1\n",
    "        series = x_window[:,last_element_index].reshape(-1)\n",
    "        cs = CubicSpline(np.arange(len(series)), series)\n",
    "        next_value = cs(len(series)+1)\n",
    "        result += [next_value]\n",
    "\n",
    "        return np.array(result).reshape(1,-1) # 1,-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Orig and Standard Scaler Transform\n",
    "## 00_DNN_pre_processing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fileDirectory + f'/data/decomposed_ticker_features_series_orig_2021-11-20_16.04.49.pkl', 'rb') as fi:\n",
    "    decomposed_ticker_features_series = pickle.load(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = list(decomposed_ticker_features_series.keys())\n",
    "years = list(decomposed_ticker_features_series['MSFT'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.04956552e-04,  1.10492236e-03, -1.05226874e-03,  4.63251260e-04,\n",
       "        6.09283340e-04,  7.21388816e-04, -3.53999003e-04,  7.59694843e-04,\n",
       "       -1.32357371e-03, -1.33479856e-03,  1.54127809e-03, -1.60359170e-03,\n",
       "        1.65011013e-03, -3.06320212e-04, -1.64421536e-03, -2.69641307e-05,\n",
       "        1.26268131e-03,  7.80107416e-04, -3.71963472e-04,  2.43447750e-04,\n",
       "       -3.66364685e-04,  5.29621167e-04, -1.89563678e-03,  1.99974061e-03,\n",
       "        2.23550885e-03, -8.74208088e-04, -1.61308458e-03,  1.18471653e-03,\n",
       "       -9.62663644e-04,  9.74920875e-04, -1.04264932e-03,  1.03783810e-03,\n",
       "       -2.39773713e-05,  7.28116205e-04, -1.62236202e-03,  1.64713657e-03,\n",
       "        1.73819460e-03, -1.28963149e-03, -1.43039917e-03, -1.18791163e-03,\n",
       "        2.62827034e-04,  7.50329243e-04, -1.08672608e-05,  7.71097268e-04,\n",
       "        1.56542455e-03, -1.77003506e-03,  6.09226467e-04,  1.66630746e-03,\n",
       "        1.43867273e-03, -3.80775032e-04, -1.02635228e-03, -1.04793066e-03,\n",
       "        1.32719001e-03, -7.54216181e-04, -1.95325720e-03,  2.16434974e-03,\n",
       "       -2.14380117e-03,  1.88392997e-03,  3.36028438e-04, -1.02441600e-03,\n",
       "       -5.05636262e-04,  3.57434224e-04, -2.46908538e-04,  3.00143399e-04,\n",
       "        2.55222614e-04, -6.09821662e-04,  7.71879604e-04, -8.73953702e-04,\n",
       "        6.01230532e-04,  9.07761488e-04,  8.92083902e-05, -1.72678951e-04,\n",
       "        7.95085777e-04, -2.80792169e-04,  6.83732719e-04, -2.67120207e-04,\n",
       "       -4.49203340e-04, -1.50954937e-03,  1.31853383e-03, -6.90128574e-04,\n",
       "       -1.75539754e-04, -7.40572652e-04,  9.15376294e-04, -9.97660137e-04,\n",
       "        1.00806302e-04, -1.84710074e-04,  8.24863242e-04,  3.90375931e-05,\n",
       "       -9.08528528e-04,  1.05667178e-03,  1.32377491e-04, -7.88263600e-04,\n",
       "       -1.57287587e-03,  4.47556035e-04,  1.49709687e-03, -1.29660133e-03,\n",
       "        1.06866578e-03,  7.04044694e-05, -7.58278658e-04,  9.10348829e-05,\n",
       "        6.13054189e-04, -6.44414153e-04,  4.33398125e-04,  4.33443652e-05,\n",
       "        1.51598860e-03, -1.78068388e-03,  1.76847799e-03, -8.16216638e-04,\n",
       "       -1.01087692e-03,  9.85814621e-04,  2.14569866e-03, -1.56161323e-03,\n",
       "       -3.08350646e-04, -5.01353026e-05, -4.32105463e-04,  1.37852660e-04,\n",
       "       -7.42808086e-04,  1.05547070e-03, -1.27798601e-03, -3.99031667e-04,\n",
       "        1.33439682e-03, -1.19062931e-03, -6.57411328e-04, -8.51387650e-04,\n",
       "        8.90078707e-04, -7.16361216e-04,  4.76747604e-04, -3.28525104e-04,\n",
       "        4.02380336e-04, -7.19405787e-04,  3.12051687e-04,  1.42988356e-03,\n",
       "        2.07985245e-03,  2.92706129e-03,  2.37421247e-03,  4.63216667e-04,\n",
       "       -3.22193091e-03,  2.73034444e-03, -2.01385844e-03,  1.30229809e-03,\n",
       "       -7.93978049e-04,  5.29647541e-04, -4.53934171e-04,  5.55005155e-04,\n",
       "       -8.78657854e-04,  1.38094030e-03, -1.82424235e-03,  1.91854079e-03,\n",
       "       -1.60311528e-03,  1.13118747e-03, -8.25106142e-04,  2.24927481e-04,\n",
       "        9.62862942e-04, -1.20266985e-03, -1.06626419e-03,  1.43728766e-03,\n",
       "       -7.67371038e-04, -1.17103797e-03,  9.35338073e-04, -5.80113318e-04,\n",
       "       -4.80558237e-04,  7.08807389e-04,  4.40906390e-04, -1.14963883e-03,\n",
       "        1.42224976e-03, -1.68654923e-03,  5.83610885e-04,  2.18990938e-03,\n",
       "       -2.42284084e-03, -1.65450205e-03,  1.06954346e-03,  2.70657528e-03,\n",
       "        2.56183386e-03, -1.14349368e-03, -2.16202361e-03,  9.58588894e-04,\n",
       "        1.29700373e-03, -9.02800341e-04,  7.16875966e-04, -7.73448579e-04,\n",
       "        1.01546770e-03,  6.79270488e-04, -1.45135202e-03,  1.56587104e-03,\n",
       "       -1.60193244e-03,  1.20483849e-03,  1.63711879e-03, -1.69403426e-03,\n",
       "        1.74979982e-03, -1.45646547e-03, -1.72225632e-03,  1.43024128e-03,\n",
       "        1.42066863e-03, -2.25691963e-05, -9.76661048e-04, -6.64084894e-04,\n",
       "        7.34292751e-04, -6.01944130e-04, -6.09606015e-04,  2.55684073e-04,\n",
       "        7.81569364e-04, -1.53946089e-04, -9.30115908e-04,  1.05458974e-03,\n",
       "       -1.26366569e-03,  1.55200695e-03, -1.79645966e-03,  1.83451988e-03,\n",
       "       -3.65129428e-04, -1.23922942e-03,  8.14657141e-04, -4.76462155e-04,\n",
       "        4.99351001e-05,  2.58151850e-04, -1.94735962e-04, -5.22277703e-04,\n",
       "       -4.61613914e-04,  9.09516379e-04,  1.02191105e-03,  1.04838046e-04,\n",
       "       -1.20713028e-03, -1.65909199e-04,  2.24764574e-04,  7.88076074e-04,\n",
       "        3.47897313e-04, -4.53891096e-04,  4.37172544e-04, -5.89083518e-04,\n",
       "        9.37447030e-05,  6.87414051e-04,  6.68414982e-04,  2.02130747e-03,\n",
       "       -1.96494407e-03,  1.50864656e-03, -2.59032973e-04,  2.05627047e-05,\n",
       "       -7.40658229e-05,  1.12805977e-04, -1.18451274e-04, -9.12285533e-04,\n",
       "       -3.20409490e-04,  1.40830511e-03,  1.20279232e-03, -7.89782170e-04,\n",
       "       -1.74629537e-03,  1.64883833e-03,  1.32609642e-03, -6.98770144e-04,\n",
       "       -1.27480361e-03,  1.14374348e-03,  1.10912247e-03, -1.19865552e-03])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of nested structure\n",
    "decomposed_ticker_features_series['MSFT'][2019]['Open']['IMF1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data organization\n",
    "## Read note about series_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_window_size = 10\n",
    "windows_sizes_for_imf_level = {\n",
    "    'IMF1': 4,\n",
    "    'IMF2': 4,\n",
    "    'IMF3': 4,\n",
    "    'IMF4': 4,\n",
    "    'IMF5': 4,\n",
    "    'IMF6': 4,\n",
    "    'IMF7': 4,\n",
    "    'IMF8': 4,\n",
    "    'Rsd': 6,\n",
    "    'DEFAULT': 4\n",
    "}\n",
    "\n",
    "target_feature_max_imf_level = {}\n",
    "\n",
    "# Coupling together the IMFs of the same level for different features to create exogenous input\n",
    "# The number of imfs for each feature decomposition may differ, thus some of the last imfs may not match in number of features\n",
    "series = {}\n",
    "for ticker in decomposed_ticker_features_series:\n",
    "    \n",
    "    series[ticker] = {}\n",
    "    target_feature_max_imf_level[ticker] ={}\n",
    "    \n",
    "    for y in decomposed_ticker_features_series[ticker]:\n",
    "        series[ticker][y] = {}\n",
    "\n",
    "        for feature in decomposed_ticker_features_series[ticker][y]:\n",
    "            \n",
    "            imfs = pd.DataFrame.from_dict(decomposed_ticker_features_series[ticker][y][feature])\n",
    "            \n",
    "            for imf in imfs:\n",
    "                if imf not in series[ticker][y]:\n",
    "                    series[ticker][y][imf] = []\n",
    "                _series = imfs[imf].values\n",
    "                _series = _series.reshape((len(_series),1)) # reshaping to get into column format\n",
    "                series[ticker][y][imf] += [_series]\n",
    "                if feature == target_feature:\n",
    "                    target_feature_max_imf_level[ticker][y] = imf\n",
    "\n",
    "# cut_spare_imfs: when any of the exogenous features have more imfs than the target feature. This solves a bug, if not excluded, these spare imfs from exogenous features would be wrongly added in the recomposition of the target feature.\n",
    "series_cut = {}\n",
    "for ticker in series:\n",
    "    if ticker not in series_cut:\n",
    "        series_cut[ticker] = {}\n",
    "    for y in series[ticker]:\n",
    "        series_cut[ticker][y] = {}\n",
    "        for imf_level_string in series[ticker][y]:\n",
    "            imf_level_int = int(imf_level_string[3:])\n",
    "            if imf_level_int > int(target_feature_max_imf_level[ticker][y][3:]):\n",
    "                continue\n",
    "            else:\n",
    "                #print(f'ticker = {str(ticker)}, y = {str(y)} imf_level_string ={imf_level_string}')\n",
    "                series_cut[ticker][y][imf_level_string] = series[ticker][y][imf_level_string].copy()\n",
    "# if doing full and not spline then uncomment below\n",
    "#series = series_cut\n",
    "\n",
    "dataset = {}\n",
    "# # horizontal stack\n",
    "for ticker in series:\n",
    "    dataset[ticker] = {}\n",
    "    for y in series[ticker]:\n",
    "        dataset[ticker][y] = {}\n",
    "        for imf_level in series[ticker][y]:\n",
    "            dataset[ticker][y][imf_level] = np.hstack(tuple(series[ticker][y][imf_level]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['IMF1', 'IMF2', 'IMF3', 'IMF4', 'IMF5', 'IMF6'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series['HD'][2020].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['IMF1', 'IMF2', 'IMF3', 'IMF4', 'IMF5', 'IMF6'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of structure\n",
    "dataset['HD'][2008].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data set split rates\n",
    "# create generators\n",
    "# NOTE STANDARD SCALER was FIT on .75 split so leakage if TRAIN VALIDATE GOES PAST\n",
    "\n",
    "train = 0.55\n",
    "validation = 0.2\n",
    "test = 0.25\n",
    "\n",
    "train_dataset = {}\n",
    "validation_dataset = {}\n",
    "test_dataset = {}\n",
    "\n",
    "train_generators = {}\n",
    "validation_generators = {}\n",
    "test_generators = {}\n",
    "\n",
    "for ticker in dataset:\n",
    "\n",
    "    train_dataset[ticker] = {}\n",
    "    validation_dataset[ticker] = {}\n",
    "    test_dataset[ticker] = {}\n",
    "\n",
    "    train_generators[ticker] = {}\n",
    "    validation_generators[ticker] = {}\n",
    "    test_generators[ticker] = {}\n",
    "    \n",
    "    for y in [2020,2021]:\n",
    "        train_dataset[ticker][y] = {}\n",
    "        validation_dataset[ticker][y] = {}\n",
    "        test_dataset[ticker][y] = {}\n",
    "\n",
    "        train_generators[ticker][y] = {}\n",
    "        validation_generators[ticker][y] = {}\n",
    "        test_generators[ticker][y] = {}\n",
    "\n",
    "        for imf_level in dataset[ticker][y]:\n",
    "            \n",
    "            # splitting data sets according to rates\n",
    "            train_dataset[ticker][y][imf_level] = dataset[ticker][y][imf_level][:round(train*dataset[ticker][y][imf_level].shape[0]),:]\n",
    "            validation_dataset[ticker][y][imf_level] = dataset[ticker][y][imf_level][round(train*dataset[ticker][y][imf_level].shape[0]):round((train+validation)*dataset[ticker][y][imf_level].shape[0]),:]\n",
    "            test_dataset[ticker][y][imf_level] = dataset[ticker][y][imf_level][round((train+validation)*dataset[ticker][y][imf_level].shape[0]):,:]\n",
    "\n",
    "            if imf_level in windows_sizes_for_imf_level:\n",
    "                window_size = windows_sizes_for_imf_level[imf_level]\n",
    "            else: \n",
    "                window_size = windows_sizes_for_imf_level['DEFAULT']\n",
    "            # windowing\n",
    "            train_generators[ticker][y][imf_level] = ManyToOneTimeSeriesGenerator(train_dataset[ticker][y][imf_level], train_dataset[ticker][y][imf_level], length=window_size, batch_size=1)\n",
    "            validation_generators[ticker][y][imf_level] = ManyToOneTimeSeriesGenerator(validation_dataset[ticker][y][imf_level], validation_dataset[ticker][y][imf_level], length=window_size, batch_size=1)\n",
    "            test_generators[ticker][y][imf_level] = ManyToOneTimeSeriesGenerator(test_dataset[ticker][y][imf_level], test_dataset[ticker][y][imf_level], length=window_size, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeating the Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model [HD][2020][IMF1]\n",
      "Training model [HD][2020][IMF2]\n",
      "Training model [ADSK][2020][IMF1]\n",
      "Training model [ADSK][2020][IMF2]\n",
      "Training model [MTD][2020][IMF1]\n",
      "Training model [MTD][2020][IMF2]\n",
      "Training model [WAT][2020][IMF1]\n",
      "Training model [WAT][2020][IMF2]\n",
      "Training model [V][2020][IMF1]\n",
      "Training model [V][2020][IMF2]\n",
      "Training model [MSFT][2020][IMF1]\n",
      "Training model [MSFT][2020][IMF2]\n",
      "Training model [CARR][2020][IMF1]\n",
      "Training model [CARR][2020][IMF2]\n",
      "Training model [AMAT][2020][IMF1]\n",
      "Training model [AMAT][2020][IMF2]\n",
      "Training model [JNJ][2020][IMF1]\n",
      "Training model [JNJ][2020][IMF2]\n",
      "Training model [UNH][2020][IMF1]\n",
      "Training model [UNH][2020][IMF2]\n",
      "Training model [XOM][2020][IMF1]\n",
      "Training model [XOM][2020][IMF2]\n"
     ]
    }
   ],
   "source": [
    "models_full = {}\n",
    "note='spline' \n",
    "spe=1 \n",
    "year=2020\n",
    "\n",
    "model_epochs = {\n",
    "    'IMF1': 2500,\n",
    "    'IMF2': 2000,\n",
    "    'IMF3': 1500,\n",
    "    'IMF4': 1500,\n",
    "    'IMF5': 1500,\n",
    "    'IMF6': 1200,\n",
    "    'IMF7': 1200,\n",
    "    'IMF8': 1000,\n",
    "    'Rsd': 1000,\n",
    "    'DEFAULT': 1000\n",
    "}\n",
    "\n",
    "imfs_to_predict_with_neural = ['IMF1','IMF2'] # set to ['IMF1'] , ['IMF1', 'IMF2'], ['IMF1','IMF2','IMF3'] and so on\n",
    "\n",
    "\n",
    "\n",
    "for ticker in train_generators:\n",
    "    models_full[ticker] = {}\n",
    "    for y in train_generators[ticker]:\n",
    "        if y == year:\n",
    "            models_full[ticker][y] = {}\n",
    "            reached_max_imf_of_target_feature = False\n",
    "            for imf_level in train_generators[ticker][y]:\n",
    "                if imf_level in imfs_to_predict_with_neural:\n",
    "                    print(f'Training model [{ticker}][{y}][{imf_level}]')\n",
    "                    if reached_max_imf_of_target_feature is True:\n",
    "                        break # no need to predict further if target feature doesn't contain greater IMF levels\n",
    "\n",
    "                    if target_feature_max_imf_level[ticker][y] == imf_level:\n",
    "                        reached_max_imf_of_target_feature = True\n",
    "                    # Prediction model\n",
    "                    model = Sequential()\n",
    "                    current_dataset = train_dataset[ticker][y][imf_level]\n",
    "                    n_features = current_dataset.shape[1]\n",
    "                    cur_tmp_gen = train_generators[ticker][y][imf_level]\n",
    "                    cur_tmp_val_gen = validation_generators[ticker][y][imf_level]\n",
    "\n",
    "                    if imf_level in windows_sizes_for_imf_level:\n",
    "                        window_size = windows_sizes_for_imf_level[imf_level]\n",
    "                    else: \n",
    "                        window_size = windows_sizes_for_imf_level['DEFAULT']\n",
    "\n",
    "                    model.add(LSTM(128, activation='tanh', return_sequences=True, input_shape=(window_size, n_features)))\n",
    "                    model.add(LSTM(64, activation='tanh', input_shape=(window_size, 128)))\n",
    "                    model.add(Dense(16))\n",
    "                    model.add(LeakyReLU())\n",
    "                    model.add(Dense(4))\n",
    "                    model.add(LeakyReLU())\n",
    "                    model.add(Dense(1)) # 1 target feature only\n",
    "                    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "                    number_of_epochs = model_epochs[imf_level]\n",
    "                    checkpoint_path = fileDirectory + \"/data/DNN_tmp/\" +f\"{ticker}_{y}_\" +f\"ltsm_{note}.h5\"\n",
    "                    \n",
    "                    callbacks = [ ModelCheckpoint(checkpoint_path, monitor='loss', mode=\"max\", verbose=0,save_best_only=True, save_weights_only=False, save_freq=250)]\n",
    "\n",
    "                    # fit model\n",
    "                    #model.fit_generator(cur_tmp_gen, steps_per_epoch=1, epochs=number_of_epochs, verbose=0)\n",
    "                    model.fit(cur_tmp_gen, validation_data=cur_tmp_val_gen, steps_per_epoch=spe, epochs=number_of_epochs, verbose=0, callbacks=callbacks)\n",
    "\n",
    "                    models_full[ticker][y][imf_level] = model\n",
    "                else:\n",
    "                    # Spline prediction model\n",
    "                    cur_tmp_gen = train_generators[ticker][y][imf_level]\n",
    "                    model = SplineModel(cur_tmp_gen)\n",
    "                    models_full[ticker][y][imf_level] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['HD', 'ADSK', 'MTD', 'WAT', 'V', 'WFC', 'MSFT', 'CARR', 'AMAT', 'JNJ', 'UNH', 'XOM'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_full.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in models_full:\n",
    "    for y in [2020]:\n",
    "        for IM in models_full[t][2020]:\n",
    "            if IM in ['IMF1','IMF2']:\n",
    "                models_full[t][y][IM].save(os.path.join(fileDirectory+f\"/data/tf_LSTM_{t}_{y}_{IM}network.h5\"))\n",
    "            else:\n",
    "                with open(fileDirectory+f'/data/LSTM_{t}_{y}_{IM}_spline.pkl', 'wb') as fi:\n",
    "                    pickle.dump(models_full[t][y][IM], fi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is notable that training a DNN for IMF 1+2 only took ~280 M for 11 stocks. Whereas training 5-6 IMFs for 11 stocks took ~765 M."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you need to reset the generators before running the following cell\n",
    "# export supporting data\n",
    "exports = {'train_dataset':train_dataset,\n",
    "               'validation_dataset':validation_dataset,\n",
    "               'test_dataset':test_dataset,\n",
    "               'train_generators':train_generators,\n",
    "               'validation_generators':validation_generators,\n",
    "               'test_generators':test_generators}\n",
    "\n",
    "for k,v in exports.items():\n",
    "    with open(fileDirectory+f'/data/LSTM_full_2nd_export_2020_models_orig_space{k}.pkl', 'wb') as fi:\n",
    "                    pickle.dump(v, fi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neither of the 1st two models did particularly well. Spline was the better of the two. Next we will do a version without the log transform and see how that does in the spline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "08348bb306580ac2ff7a5930f57ccf23fdbb79f7c6aaa7428d52776827adbd16"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('other': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
