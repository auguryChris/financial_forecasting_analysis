{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta, datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from scipy.interpolate import CubicSpline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, LeakyReLU\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tqdm import tqdm\n",
    "import sys, os\n",
    "\n",
    "experiment_time = datetime.now().strftime(\"%H_%M_%S_%m_%d_%Y\")\n",
    "\n",
    "\n",
    "features_in_order = ['Open', 'High', 'Low', 'Volume', 'Close'] # target feature must be the last one here\n",
    "target_feature = 'Close'\n",
    "\n",
    "absolutepath = os.path.abspath('')\n",
    "fileDirectory = os.path.dirname(absolutepath)\n",
    "\n",
    "class ManyToOneTimeSeriesGenerator(TimeseriesGenerator):\n",
    "  def __getitem__(self, idx):\n",
    "    x, y = super().__getitem__(idx)\n",
    "    last_element_index = y.shape[1]-1\n",
    "    return x, y[:,last_element_index].reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'22_00_50_11_19_2021'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.19.5'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to model later IMFs as splines\n",
    "class SplineModel():\n",
    "    def __init__(self,time_series_generator):\n",
    "        self.name = \"SplineModel\"\n",
    "        self.gen = time_series_generator\n",
    "    \n",
    "    def predict(self, x_window, verbose=0):\n",
    "        result = []\n",
    "        x_window = np.squeeze(x_window, axis=0)\n",
    "        last_element_index = x_window.shape[1]-1\n",
    "        series = x_window[:,last_element_index].reshape(-1)\n",
    "        cs = CubicSpline(np.arange(len(series)), series)\n",
    "        next_value = cs(len(series)+1)\n",
    "        result += [next_value]\n",
    "\n",
    "        return np.array(result).reshape(1,-1) # 1,-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Log Transformed and Standard Scaler Transform\n",
    "## 00_DNN_pre_processing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fileDirectory + f'/data/std_scale_trans_75_25_split_on_year_decomposed_ticker_features_series_log_2021-11-17_15.44.40.pkl', 'rb') as fi:\n",
    "    decomposed_ticker_features_series = pickle.load(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = list(decomposed_ticker_features_series.keys())\n",
    "years = list(decomposed_ticker_features_series['MSFT'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.13038013e-04,  1.10429028e-03, -1.03732709e-03,  3.46936006e-04,\n",
       "        5.83459829e-04,  6.62470565e-04, -4.13004284e-04,  7.40215529e-04,\n",
       "       -1.27108983e-03, -1.29599952e-03,  1.52096065e-03, -1.59589046e-03,\n",
       "        1.64420873e-03, -3.02100855e-04, -1.61368830e-03, -2.04555935e-04,\n",
       "        1.15085746e-03,  6.34560561e-04, -4.16424978e-04,  2.41264601e-04,\n",
       "       -3.42745297e-04,  8.82930959e-04, -1.66253006e-03,  2.12253514e-03,\n",
       "        2.09916049e-03, -9.82567989e-04, -1.64631056e-03,  1.23345787e-03,\n",
       "       -9.84824794e-04,  9.59186698e-04, -1.03347013e-03,  1.09025327e-03,\n",
       "        1.23361530e-04,  8.07760606e-04, -1.68340386e-03,  1.52141277e-03,\n",
       "        1.68938839e-03, -1.33843530e-03, -1.54047158e-03, -1.15936202e-03,\n",
       "        3.29083489e-04,  7.73011582e-04,  5.83768969e-06,  8.35053884e-04,\n",
       "        1.60440994e-03, -1.78162307e-03,  6.02642137e-04,  1.70160447e-03,\n",
       "        1.37920319e-03, -4.52730803e-04, -1.07939121e-03, -1.14213071e-03,\n",
       "        1.35218903e-03, -7.27875722e-04, -1.94742526e-03,  2.15960800e-03,\n",
       "       -2.15296436e-03,  1.91100043e-03,  2.67255428e-04, -1.03888639e-03,\n",
       "       -5.26174086e-04,  3.83302500e-04, -2.60199995e-04,  2.93264377e-04,\n",
       "        3.59992813e-04, -6.25771325e-04,  7.88191378e-04, -8.69157651e-04,\n",
       "        5.83978925e-04,  7.98591129e-04, -8.21362684e-05, -3.18472400e-04,\n",
       "        5.81799106e-04, -3.62531465e-04,  5.78456033e-04, -1.44150596e-04,\n",
       "       -3.15594797e-05, -1.27416534e-03,  1.28050809e-03, -9.35997308e-04,\n",
       "       -3.30677023e-04, -7.33080620e-04,  9.23290926e-04, -8.94169228e-04,\n",
       "        2.79644163e-04, -2.50404401e-04,  6.59506927e-04, -9.09530959e-05,\n",
       "       -9.05655479e-04,  1.10709778e-03,  1.78254238e-04, -7.31611079e-04,\n",
       "       -1.56881243e-03,  3.83181633e-04,  1.44462312e-03, -1.26185608e-03,\n",
       "        1.07461895e-03,  1.14479354e-04, -8.16929702e-04,  4.23632988e-05,\n",
       "        6.03867776e-04, -5.65401252e-04,  3.78488539e-04, -1.83836280e-04,\n",
       "        1.41574938e-03, -1.67401297e-03,  1.68618033e-03, -1.11424607e-03,\n",
       "       -1.62519977e-03,  4.09661197e-04,  1.73099445e-03, -1.47753850e-03,\n",
       "        5.09550178e-05,  3.17972325e-04, -2.33139456e-04,  3.44141332e-04,\n",
       "       -6.33628816e-04,  1.05816443e-03, -1.33127126e-03, -4.32524529e-04,\n",
       "        1.39876531e-03, -1.21703529e-03, -7.38004683e-04, -8.59003939e-04,\n",
       "        8.43417017e-04, -6.64061136e-04,  4.52064700e-04, -3.50016586e-04,\n",
       "        4.70859377e-04, -8.21233878e-04,  2.55953914e-04,  1.36819917e-03,\n",
       "        2.09883178e-03,  2.99466825e-03,  2.42563319e-03,  5.68465347e-04,\n",
       "       -3.19582072e-03,  2.68558199e-03, -1.96288798e-03,  1.25597181e-03,\n",
       "       -7.57175819e-04,  5.01525055e-04, -4.34216578e-04,  5.47398947e-04,\n",
       "       -8.82747383e-04,  1.38444344e-03, -1.81390195e-03,  1.89742490e-03,\n",
       "       -1.58566021e-03,  1.12273151e-03, -8.18055855e-04,  2.09586628e-04,\n",
       "        9.43229571e-04, -1.22002021e-03, -1.07962688e-03,  1.43923899e-03,\n",
       "       -8.04134675e-04, -1.20857495e-03,  9.86304644e-04, -5.16961879e-04,\n",
       "       -6.01552041e-04,  7.32108382e-04,  5.12012381e-04, -1.14602338e-03,\n",
       "        1.42421256e-03, -1.69582461e-03,  5.18567843e-04,  2.19771400e-03,\n",
       "       -2.42374717e-03, -1.69163659e-03,  1.08987017e-03,  2.73355295e-03,\n",
       "        2.54726703e-03, -1.10736491e-03, -2.10660284e-03,  9.69926303e-04,\n",
       "        1.25696050e-03, -8.91810019e-04,  7.12156499e-04, -7.68252817e-04,\n",
       "        9.85283523e-04,  6.91111610e-04, -1.45884352e-03,  1.57641972e-03,\n",
       "       -1.61281210e-03,  1.22023976e-03,  1.64267398e-03, -1.69188538e-03,\n",
       "        1.73901255e-03, -1.41554213e-03, -1.69373288e-03,  1.48885501e-03,\n",
       "        1.40565955e-03,  9.26307906e-06, -9.72350859e-04, -7.15536173e-04,\n",
       "        7.54834701e-04, -6.04907809e-04, -6.08024863e-04,  2.23647496e-04,\n",
       "        7.75029501e-04, -2.70157831e-04, -9.18444555e-04,  1.05538741e-03,\n",
       "       -1.27843371e-03,  1.56524362e-03, -1.78957455e-03,  1.80586699e-03,\n",
       "       -2.74122372e-04, -1.20643962e-03,  7.94984429e-04, -4.71666527e-04,\n",
       "        2.71685305e-05,  2.71084436e-04, -1.71905841e-04, -5.55332722e-04,\n",
       "       -4.40373650e-04,  7.85422370e-04,  1.01332348e-03,  2.54269042e-04,\n",
       "       -1.04253878e-03, -1.04134606e-04,  1.39788649e-04,  6.52083694e-04,\n",
       "        1.46743447e-04, -4.40085683e-04,  4.71937948e-04, -6.46497003e-04,\n",
       "        1.22049401e-05,  5.90978132e-04,  5.75310359e-04,  1.93341842e-03,\n",
       "       -1.89210019e-03,  1.57891726e-03, -1.70653115e-04,  1.59720340e-04,\n",
       "       -1.25588242e-04, -6.27878392e-05, -2.50888713e-04, -9.73968074e-04,\n",
       "       -4.62304810e-04,  1.37244165e-03,  1.17674113e-03, -7.21951296e-04,\n",
       "       -1.70070695e-03,  1.61182329e-03,  1.31413012e-03, -7.74913653e-04,\n",
       "       -1.27806084e-03,  1.17755414e-03,  1.06864384e-03, -1.21589863e-03])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of nested structure\n",
    "decomposed_ticker_features_series['MSFT'][2019]['Open']['IMF1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data organization\n",
    "## Read note about series_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_window_size = 10\n",
    "windows_sizes_for_imf_level = {\n",
    "    'IMF1': 4,\n",
    "    'IMF2': 4,\n",
    "    'IMF3': 4,\n",
    "    'IMF4': 4,\n",
    "    'IMF5': 4,\n",
    "    'IMF6': 4,\n",
    "    'IMF7': 4,\n",
    "    'IMF8': 4,\n",
    "    'Rsd': 6,\n",
    "    'DEFAULT': 4\n",
    "}\n",
    "\n",
    "target_feature_max_imf_level = {}\n",
    "\n",
    "# Coupling together the IMFs of the same level for different features to create exogenous input\n",
    "# The number of imfs for each feature decomposition may differ, thus some of the last imfs may not match in number of features\n",
    "series = {}\n",
    "for ticker in decomposed_ticker_features_series:\n",
    "    \n",
    "    series[ticker] = {}\n",
    "    target_feature_max_imf_level[ticker] ={}\n",
    "    \n",
    "    for y in decomposed_ticker_features_series[ticker]:\n",
    "        series[ticker][y] = {}\n",
    "\n",
    "        for feature in decomposed_ticker_features_series[ticker][y]:\n",
    "            \n",
    "            imfs = pd.DataFrame.from_dict(decomposed_ticker_features_series[ticker][y][feature])\n",
    "            \n",
    "            for imf in imfs:\n",
    "                if imf not in series[ticker][y]:\n",
    "                    series[ticker][y][imf] = []\n",
    "                _series = imfs[imf].values\n",
    "                _series = _series.reshape((len(_series),1)) # reshaping to get into column format\n",
    "                series[ticker][y][imf] += [_series]\n",
    "                if feature == target_feature:\n",
    "                    target_feature_max_imf_level[ticker][y] = imf\n",
    "\n",
    "# cut_spare_imfs: when any of the exogenous features have more imfs than the target feature. This solves a bug, if not excluded, these spare imfs from exogenous features would be wrongly added in the recomposition of the target feature.\n",
    "series_cut = {}\n",
    "for ticker in series:\n",
    "    if ticker not in series_cut:\n",
    "        series_cut[ticker] = {}\n",
    "    for y in series[ticker]:\n",
    "        series_cut[ticker][y] = {}\n",
    "        for imf_level_string in series[ticker][y]:\n",
    "            imf_level_int = int(imf_level_string[3:])\n",
    "            if imf_level_int > int(target_feature_max_imf_level[ticker][y][3:]):\n",
    "                continue\n",
    "            else:\n",
    "                #print(f'ticker = {str(ticker)}, y = {str(y)} imf_level_string ={imf_level_string}')\n",
    "                series_cut[ticker][y][imf_level_string] = series[ticker][y][imf_level_string].copy()\n",
    "# if doing full and not spline then uncomment below\n",
    "series = series_cut\n",
    "\n",
    "dataset = {}\n",
    "# # horizontal stack\n",
    "for ticker in series:\n",
    "    dataset[ticker] = {}\n",
    "    for y in series[ticker]:\n",
    "        dataset[ticker][y] = {}\n",
    "        for imf_level in series[ticker][y]:\n",
    "            dataset[ticker][y][imf_level] = np.hstack(tuple(series[ticker][y][imf_level]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['IMF1', 'IMF2', 'IMF3', 'IMF4', 'IMF5', 'IMF6', 'IMF7'])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series['HD'][2020].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['IMF1', 'IMF2', 'IMF3', 'IMF4', 'IMF5', 'IMF6'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of structure\n",
    "dataset['HD'][2008].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data set split rates\n",
    "# create generators\n",
    "# NOTE STANDARD SCALER was FIT on .75 split so leakage if TRAIN VALIDATE GOES PAST\n",
    "\n",
    "train = 0.55\n",
    "validation = 0.2\n",
    "test = 0.25\n",
    "\n",
    "train_dataset = {}\n",
    "validation_dataset = {}\n",
    "test_dataset = {}\n",
    "\n",
    "train_generators = {}\n",
    "validation_generators = {}\n",
    "test_generators = {}\n",
    "\n",
    "for ticker in dataset:\n",
    "\n",
    "    train_dataset[ticker] = {}\n",
    "    validation_dataset[ticker] = {}\n",
    "    test_dataset[ticker] = {}\n",
    "\n",
    "    train_generators[ticker] = {}\n",
    "    validation_generators[ticker] = {}\n",
    "    test_generators[ticker] = {}\n",
    "    \n",
    "    for y in [2020,2021]:\n",
    "        train_dataset[ticker][y] = {}\n",
    "        validation_dataset[ticker][y] = {}\n",
    "        test_dataset[ticker][y] = {}\n",
    "\n",
    "        train_generators[ticker][y] = {}\n",
    "        validation_generators[ticker][y] = {}\n",
    "        test_generators[ticker][y] = {}\n",
    "\n",
    "        for imf_level in dataset[ticker][y]:\n",
    "            \n",
    "            # splitting data sets according to rates\n",
    "            train_dataset[ticker][y][imf_level] = dataset[ticker][y][imf_level][:round(train*dataset[ticker][y][imf_level].shape[0]),:]\n",
    "            validation_dataset[ticker][y][imf_level] = dataset[ticker][y][imf_level][round(train*dataset[ticker][y][imf_level].shape[0]):round((train+validation)*dataset[ticker][y][imf_level].shape[0]),:]\n",
    "            test_dataset[ticker][y][imf_level] = dataset[ticker][y][imf_level][round((train+validation)*dataset[ticker][y][imf_level].shape[0]):,:]\n",
    "\n",
    "            if imf_level in windows_sizes_for_imf_level:\n",
    "                window_size = windows_sizes_for_imf_level[imf_level]\n",
    "            else: \n",
    "                window_size = windows_sizes_for_imf_level['DEFAULT']\n",
    "            # windowing\n",
    "            train_generators[ticker][y][imf_level] = ManyToOneTimeSeriesGenerator(train_dataset[ticker][y][imf_level], train_dataset[ticker][y][imf_level], length=window_size, batch_size=1)\n",
    "            validation_generators[ticker][y][imf_level] = ManyToOneTimeSeriesGenerator(validation_dataset[ticker][y][imf_level], validation_dataset[ticker][y][imf_level], length=window_size, batch_size=1)\n",
    "            test_generators[ticker][y][imf_level] = ManyToOneTimeSeriesGenerator(test_dataset[ticker][y][imf_level], test_dataset[ticker][y][imf_level], length=window_size, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_feature_max_imf_level['HD'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model [HD][2021][IMF1]\n",
      "Training model [HD][2021][IMF2]\n",
      "Training model [ADSK][2021][IMF1]\n",
      "Training model [ADSK][2021][IMF2]\n",
      "Training model [MTD][2021][IMF1]\n",
      "Training model [MTD][2021][IMF2]\n",
      "Training model [WAT][2021][IMF1]\n",
      "Training model [WAT][2021][IMF2]\n",
      "Training model [V][2021][IMF1]\n",
      "Training model [V][2021][IMF2]\n",
      "Training model [MSFT][2021][IMF1]\n",
      "Training model [MSFT][2021][IMF2]\n",
      "Training model [CARR][2021][IMF1]\n",
      "Training model [CARR][2021][IMF2]\n",
      "Training model [AMAT][2021][IMF1]\n",
      "Training model [AMAT][2021][IMF2]\n",
      "Training model [JNJ][2021][IMF1]\n",
      "Training model [JNJ][2021][IMF2]\n",
      "Training model [UNH][2021][IMF1]\n",
      "Training model [UNH][2021][IMF2]\n",
      "Training model [XOM][2021][IMF1]\n",
      "Training model [XOM][2021][IMF2]\n"
     ]
    }
   ],
   "source": [
    "# Model Training\n",
    "\n",
    "models = {}\n",
    "\n",
    "model_epochs = {\n",
    "    'IMF1': 2500,\n",
    "    'IMF2': 2000,\n",
    "    'IMF3': 1500,\n",
    "    'IMF4': 1500,\n",
    "    'IMF5': 1500,\n",
    "    'IMF6': 1200,\n",
    "    'IMF7': 1200,\n",
    "    'IMF8': 1000,\n",
    "    'Rsd': 1000,\n",
    "    'DEFAULT': 1000\n",
    "}\n",
    "\n",
    "imfs_to_predict_with_neural = ['IMF1', 'IMF2'] # set to ['IMF1'] , ['IMF1', 'IMF2'], ['IMF1','IMF2','IMF3'] and so on\n",
    "\n",
    "\n",
    "\n",
    "for ticker in train_generators:\n",
    "    models[ticker] = {}\n",
    "    for y in train_generators[ticker]:\n",
    "        models[ticker][y] = {}\n",
    "        reached_max_imf_of_target_feature = False\n",
    "        for imf_level in train_generators[ticker][y]:\n",
    "            if imf_level in imfs_to_predict_with_neural:\n",
    "                print(f'Training model [{ticker}][{y}][{imf_level}]')\n",
    "                if reached_max_imf_of_target_feature is True:\n",
    "                    break # no need to predict further if target feature doesn't contain greater IMF levels\n",
    "\n",
    "                if target_feature_max_imf_level[ticker][y] == imf_level:\n",
    "                    reached_max_imf_of_target_feature = True\n",
    "                # Prediction model\n",
    "                model = Sequential()\n",
    "                current_dataset = train_dataset[ticker][y][imf_level]\n",
    "                n_features = current_dataset.shape[1]\n",
    "                cur_tmp_gen = train_generators[ticker][y][imf_level]\n",
    "                cur_tmp_val_gen = validation_generators[ticker][y][imf_level]\n",
    "\n",
    "                if imf_level in windows_sizes_for_imf_level:\n",
    "                    window_size = windows_sizes_for_imf_level[imf_level]\n",
    "                else: \n",
    "                    window_size = windows_sizes_for_imf_level['DEFAULT']\n",
    "\n",
    "                model.add(LSTM(128, activation='tanh', return_sequences=True, input_shape=(window_size, n_features)))\n",
    "                model.add(LSTM(64, activation='tanh', input_shape=(window_size, 128)))\n",
    "                model.add(Dense(16))\n",
    "                model.add(LeakyReLU())\n",
    "                model.add(Dense(4))\n",
    "                model.add(LeakyReLU())\n",
    "                model.add(Dense(1)) # 1 target feature only\n",
    "                model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "                number_of_epochs = model_epochs[imf_level]\n",
    "                checkpoint_path = fileDirectory + \"/data/DNN_tmp/\" +f\"{ticker}_{y}_\" +f\"ltsm_spline.h5\"\n",
    "                \n",
    "                callbacks = [ ModelCheckpoint(checkpoint_path, monitor='loss', mode=\"max\", verbose=0,save_best_only=True, save_weights_only=False, save_freq=250)]\n",
    "\n",
    "                # fit model\n",
    "                #model.fit_generator(cur_tmp_gen, steps_per_epoch=1, epochs=number_of_epochs, verbose=0)\n",
    "                model.fit(cur_tmp_gen, validation_data=cur_tmp_val_gen, steps_per_epoch=10, epochs=number_of_epochs, verbose=0, callbacks=callbacks)\n",
    "\n",
    "                models[ticker][y][imf_level] = model\n",
    "            else:\n",
    "                # Spline prediction model\n",
    "                cur_tmp_gen = train_generators[ticker][y][imf_level]\n",
    "                model = SplineModel(cur_tmp_gen)\n",
    "                models[ticker][y][imf_level] = model\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'IMF1': <keras.engine.sequential.Sequential at 0x294626460>,\n",
       " 'IMF2': <keras.engine.sequential.Sequential at 0x282db36d0>,\n",
       " 'IMF3': <__main__.SplineModel at 0x2903196d0>,\n",
       " 'IMF4': <__main__.SplineModel at 0x2cd715220>,\n",
       " 'IMF5': <__main__.SplineModel at 0x2cd715d30>,\n",
       " 'IMF6': <__main__.SplineModel at 0x2cd715d00>}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example \n",
    "models['HD'][2021]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in models:\n",
    "    for y in [2021]:\n",
    "        for IM in models[t][2021]:\n",
    "            if IM in ['IMF1','IMF2']:\n",
    "                models[t][y][IM].save(os.path.join(fileDirectory+f\"/data/tf_LSTM_{t}_{y}_{IM}network.h5\"))\n",
    "            else:\n",
    "                with open(fileDirectory+f'/data/LSTM_{t}_{y}_{IM}_spline.pkl', 'wb') as fi:\n",
    "                    pickle.dump(models[t][y][IM], fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export supporting data\n",
    "exports = {'train_dataset':train_dataset,\n",
    "               'validation_dataset':validation_dataset,\n",
    "               'test_dataset':test_dataset,\n",
    "               'train_generators':train_generators,\n",
    "               'validation_generators':validation_generators,\n",
    "               'test_generators':test_generators}\n",
    "\n",
    "for k,v in exports.items():\n",
    "    with open(fileDirectory+f'/data/LTSM_1st_export_all_tickers_2021_{k}.pkl', 'wb') as fi:\n",
    "                    pickle.dump(v, fi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeating the Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_trainer(imfs_to_predict=['IMF1','IMF2','IMF3','IMF4','IMF5','IMF6','IMF7','IMF8','RSD'],note='full', spe=1, year=2020):\n",
    "    # Model Training\n",
    "\n",
    "    models_full = {}\n",
    "\n",
    "    model_epochs = {\n",
    "        'IMF1': 2500,\n",
    "        'IMF2': 2000,\n",
    "        'IMF3': 1500,\n",
    "        'IMF4': 1500,\n",
    "        'IMF5': 1500,\n",
    "        'IMF6': 1200,\n",
    "        'IMF7': 1200,\n",
    "        'IMF8': 1000,\n",
    "        'Rsd': 1000,\n",
    "        'DEFAULT': 1000\n",
    "    }\n",
    "\n",
    "    imfs_to_predict_with_neural = imfs_to_predict # set to ['IMF1'] , ['IMF1', 'IMF2'], ['IMF1','IMF2','IMF3'] and so on\n",
    "\n",
    "\n",
    "\n",
    "    for ticker in train_generators:\n",
    "        models_full[ticker] = {}\n",
    "        for y in train_generators[ticker]:\n",
    "            if y == year:\n",
    "                models_full[ticker][y] = {}\n",
    "                reached_max_imf_of_target_feature = False\n",
    "                for imf_level in train_generators[ticker][y]:\n",
    "                    if imf_level in imfs_to_predict_with_neural:\n",
    "                        print(f'Training model [{ticker}][{y}][{imf_level}]')\n",
    "                        if reached_max_imf_of_target_feature is True:\n",
    "                            break # no need to predict further if target feature doesn't contain greater IMF levels\n",
    "\n",
    "                        if target_feature_max_imf_level[ticker][y] == imf_level:\n",
    "                            reached_max_imf_of_target_feature = True\n",
    "                        # Prediction model\n",
    "                        model = Sequential()\n",
    "                        current_dataset = train_dataset[ticker][y][imf_level]\n",
    "                        n_features = current_dataset.shape[1]\n",
    "                        cur_tmp_gen = train_generators[ticker][y][imf_level]\n",
    "                        cur_tmp_val_gen = validation_generators[ticker][y][imf_level]\n",
    "\n",
    "                        if imf_level in windows_sizes_for_imf_level:\n",
    "                            window_size = windows_sizes_for_imf_level[imf_level]\n",
    "                        else: \n",
    "                            window_size = windows_sizes_for_imf_level['DEFAULT']\n",
    "\n",
    "                        model.add(LSTM(128, activation='tanh', return_sequences=True, input_shape=(window_size, n_features)))\n",
    "                        model.add(LSTM(64, activation='tanh', input_shape=(window_size, 128)))\n",
    "                        model.add(Dense(16))\n",
    "                        model.add(LeakyReLU())\n",
    "                        model.add(Dense(4))\n",
    "                        model.add(LeakyReLU())\n",
    "                        model.add(Dense(1)) # 1 target feature only\n",
    "                        model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "                        number_of_epochs = model_epochs[imf_level]\n",
    "                        checkpoint_path = fileDirectory + \"/data/DNN_tmp/\" +f\"{ticker}_{y}_\" +f\"ltsm_{note}.h5\"\n",
    "                        \n",
    "                        callbacks = [ ModelCheckpoint(checkpoint_path, monitor='loss', mode=\"max\", verbose=0,save_best_only=True, save_weights_only=False, save_freq=250)]\n",
    "\n",
    "                        # fit model\n",
    "                        #model.fit_generator(cur_tmp_gen, steps_per_epoch=1, epochs=number_of_epochs, verbose=0)\n",
    "                        model.fit(cur_tmp_gen, validation_data=cur_tmp_val_gen, steps_per_epoch=spe, epochs=number_of_epochs, verbose=0, callbacks=callbacks)\n",
    "\n",
    "                        models_full[ticker][y][imf_level] = model\n",
    "                    else:\n",
    "                        # Spline prediction model\n",
    "                        cur_tmp_gen = train_generators[ticker][y][imf_level]\n",
    "                        model = SplineModel(cur_tmp_gen)\n",
    "                        models_full[ticker][y][imf_level] = model\n",
    "    \n",
    "    return models_full\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model [HD][2020][IMF1]\n",
      "Training model [HD][2020][IMF2]\n",
      "Training model [HD][2020][IMF3]\n",
      "Training model [HD][2020][IMF4]\n",
      "Training model [HD][2020][IMF5]\n",
      "Training model [HD][2020][IMF6]\n",
      "Training model [ADSK][2020][IMF1]\n",
      "Training model [ADSK][2020][IMF2]\n",
      "Training model [ADSK][2020][IMF3]\n",
      "Training model [ADSK][2020][IMF4]\n",
      "Training model [ADSK][2020][IMF5]\n",
      "Training model [ADSK][2020][IMF6]\n",
      "Training model [MTD][2020][IMF1]\n",
      "Training model [MTD][2020][IMF2]\n",
      "Training model [MTD][2020][IMF3]\n",
      "Training model [MTD][2020][IMF4]\n",
      "Training model [MTD][2020][IMF5]\n",
      "Training model [WAT][2020][IMF1]\n",
      "Training model [WAT][2020][IMF2]\n",
      "Training model [WAT][2020][IMF3]\n",
      "Training model [WAT][2020][IMF4]\n",
      "Training model [WAT][2020][IMF5]\n",
      "Training model [WAT][2020][IMF6]\n",
      "Training model [V][2020][IMF1]\n",
      "Training model [V][2020][IMF2]\n",
      "Training model [V][2020][IMF3]\n",
      "Training model [V][2020][IMF4]\n",
      "Training model [V][2020][IMF5]\n",
      "Training model [V][2020][IMF6]\n",
      "Training model [MSFT][2020][IMF1]\n",
      "Training model [MSFT][2020][IMF2]\n",
      "Training model [MSFT][2020][IMF3]\n",
      "Training model [MSFT][2020][IMF4]\n",
      "Training model [MSFT][2020][IMF5]\n",
      "Training model [MSFT][2020][IMF6]\n",
      "Training model [CARR][2020][IMF1]\n",
      "Training model [CARR][2020][IMF2]\n",
      "Training model [CARR][2020][IMF3]\n",
      "Training model [CARR][2020][IMF4]\n"
     ]
    }
   ],
   "source": [
    "models_full = model_trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model [HD][2020][IMF1]\n",
      "Training model [HD][2020][IMF2]\n",
      "Training model [HD][2020][IMF3]\n",
      "Training model [HD][2020][IMF4]\n",
      "Training model [HD][2020][IMF5]\n",
      "Training model [HD][2020][IMF6]\n",
      "Training model [ADSK][2020][IMF1]\n",
      "Training model [ADSK][2020][IMF2]\n",
      "Training model [ADSK][2020][IMF3]\n",
      "Training model [ADSK][2020][IMF4]\n",
      "Training model [ADSK][2020][IMF5]\n",
      "Training model [ADSK][2020][IMF6]\n",
      "Training model [MTD][2020][IMF1]\n",
      "Training model [MTD][2020][IMF2]\n",
      "Training model [MTD][2020][IMF3]\n",
      "Training model [MTD][2020][IMF4]\n",
      "Training model [MTD][2020][IMF5]\n",
      "Training model [WAT][2020][IMF1]\n",
      "Training model [WAT][2020][IMF2]\n",
      "Training model [WAT][2020][IMF3]\n",
      "Training model [WAT][2020][IMF4]\n",
      "Training model [WAT][2020][IMF5]\n",
      "Training model [WAT][2020][IMF6]\n",
      "Training model [V][2020][IMF1]\n",
      "Training model [V][2020][IMF2]\n",
      "Training model [V][2020][IMF3]\n",
      "Training model [V][2020][IMF4]\n",
      "Training model [V][2020][IMF5]\n",
      "Training model [V][2020][IMF6]\n",
      "Training model [MSFT][2020][IMF1]\n",
      "Training model [MSFT][2020][IMF2]\n",
      "Training model [MSFT][2020][IMF3]\n",
      "Training model [MSFT][2020][IMF4]\n",
      "Training model [MSFT][2020][IMF5]\n",
      "Training model [MSFT][2020][IMF6]\n",
      "Training model [CARR][2020][IMF1]\n",
      "Training model [CARR][2020][IMF2]\n",
      "Training model [CARR][2020][IMF3]\n",
      "Training model [CARR][2020][IMF4]\n",
      "Training model [CARR][2020][IMF5]\n",
      "Training model [AMAT][2020][IMF1]\n",
      "Training model [AMAT][2020][IMF2]\n",
      "Training model [AMAT][2020][IMF3]\n",
      "Training model [AMAT][2020][IMF4]\n",
      "Training model [AMAT][2020][IMF5]\n",
      "Training model [AMAT][2020][IMF6]\n",
      "Training model [JNJ][2020][IMF1]\n",
      "Training model [JNJ][2020][IMF2]\n",
      "Training model [JNJ][2020][IMF3]\n",
      "Training model [JNJ][2020][IMF4]\n",
      "Training model [JNJ][2020][IMF5]\n",
      "Training model [JNJ][2020][IMF6]\n",
      "Training model [UNH][2020][IMF1]\n",
      "Training model [UNH][2020][IMF2]\n",
      "Training model [UNH][2020][IMF3]\n",
      "Training model [UNH][2020][IMF4]\n",
      "Training model [UNH][2020][IMF5]\n",
      "Training model [UNH][2020][IMF6]\n",
      "Training model [XOM][2020][IMF1]\n",
      "Training model [XOM][2020][IMF2]\n",
      "Training model [XOM][2020][IMF3]\n",
      "Training model [XOM][2020][IMF4]\n",
      "Training model [XOM][2020][IMF5]\n",
      "Training model [XOM][2020][IMF6]\n"
     ]
    }
   ],
   "source": [
    "models_full = {}\n",
    "note='full' \n",
    "spe=1 \n",
    "year=2020\n",
    "\n",
    "model_epochs = {\n",
    "    'IMF1': 2500,\n",
    "    'IMF2': 2000,\n",
    "    'IMF3': 1500,\n",
    "    'IMF4': 1500,\n",
    "    'IMF5': 1500,\n",
    "    'IMF6': 1200,\n",
    "    'IMF7': 1200,\n",
    "    'IMF8': 1000,\n",
    "    'Rsd': 1000,\n",
    "    'DEFAULT': 1000\n",
    "}\n",
    "\n",
    "imfs_to_predict_with_neural = ['IMF1','IMF2','IMF3','IMF4','IMF5','IMF6','IMF7','IMF8','RSD'] # set to ['IMF1'] , ['IMF1', 'IMF2'], ['IMF1','IMF2','IMF3'] and so on\n",
    "\n",
    "\n",
    "\n",
    "for ticker in train_generators:\n",
    "    models_full[ticker] = {}\n",
    "    for y in train_generators[ticker]:\n",
    "        if y == year:\n",
    "            models_full[ticker][y] = {}\n",
    "            reached_max_imf_of_target_feature = False\n",
    "            for imf_level in train_generators[ticker][y]:\n",
    "                if imf_level in imfs_to_predict_with_neural:\n",
    "                    print(f'Training model [{ticker}][{y}][{imf_level}]')\n",
    "                    if reached_max_imf_of_target_feature is True:\n",
    "                        break # no need to predict further if target feature doesn't contain greater IMF levels\n",
    "\n",
    "                    if target_feature_max_imf_level[ticker][y] == imf_level:\n",
    "                        reached_max_imf_of_target_feature = True\n",
    "                    # Prediction model\n",
    "                    model = Sequential()\n",
    "                    current_dataset = train_dataset[ticker][y][imf_level]\n",
    "                    n_features = current_dataset.shape[1]\n",
    "                    cur_tmp_gen = train_generators[ticker][y][imf_level]\n",
    "                    cur_tmp_val_gen = validation_generators[ticker][y][imf_level]\n",
    "\n",
    "                    if imf_level in windows_sizes_for_imf_level:\n",
    "                        window_size = windows_sizes_for_imf_level[imf_level]\n",
    "                    else: \n",
    "                        window_size = windows_sizes_for_imf_level['DEFAULT']\n",
    "\n",
    "                    model.add(LSTM(128, activation='tanh', return_sequences=True, input_shape=(window_size, n_features)))\n",
    "                    model.add(LSTM(64, activation='tanh', input_shape=(window_size, 128)))\n",
    "                    model.add(Dense(16))\n",
    "                    model.add(LeakyReLU())\n",
    "                    model.add(Dense(4))\n",
    "                    model.add(LeakyReLU())\n",
    "                    model.add(Dense(1)) # 1 target feature only\n",
    "                    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "                    number_of_epochs = model_epochs[imf_level]\n",
    "                    checkpoint_path = fileDirectory + \"/data/DNN_tmp/\" +f\"{ticker}_{y}_\" +f\"ltsm_{note}.h5\"\n",
    "                    \n",
    "                    callbacks = [ ModelCheckpoint(checkpoint_path, monitor='loss', mode=\"max\", verbose=0,save_best_only=True, save_weights_only=False, save_freq=250)]\n",
    "\n",
    "                    # fit model\n",
    "                    #model.fit_generator(cur_tmp_gen, steps_per_epoch=1, epochs=number_of_epochs, verbose=0)\n",
    "                    model.fit(cur_tmp_gen, validation_data=cur_tmp_val_gen, steps_per_epoch=spe, epochs=number_of_epochs, verbose=0, callbacks=callbacks)\n",
    "\n",
    "                    models_full[ticker][y][imf_level] = model\n",
    "                else:\n",
    "                    # Spline prediction model\n",
    "                    cur_tmp_gen = train_generators[ticker][y][imf_level]\n",
    "                    model = SplineModel(cur_tmp_gen)\n",
    "                    models_full[ticker][y][imf_level] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['HD', 'ADSK', 'MTD', 'WAT', 'V', 'WFC', 'MSFT', 'CARR', 'AMAT', 'JNJ', 'UNH', 'XOM'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_full.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in models_full:\n",
    "    for y in [2020]:\n",
    "        for IM in models_full[t][2020]:\n",
    "            models_full[t][y][IM].save(os.path.join(fileDirectory+f\"/data/tf_LSTM_full_{t}_{y}_{IM}.h5\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is notable that training a DNN for IMF 1+2 only took ~280 M for 11 stocks. Whereas training 5-6 IMFs for 11 stocks took ~765 M."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you need to reset the generators before running the following cell\n",
    "# export supporting data\n",
    "exports = {'train_dataset':train_dataset,\n",
    "               'validation_dataset':validation_dataset,\n",
    "               'test_dataset':test_dataset,\n",
    "               'train_generators':train_generators,\n",
    "               'validation_generators':validation_generators,\n",
    "               'test_generators':test_generators}\n",
    "\n",
    "for k,v in exports.items():\n",
    "    with open(fileDirectory+f'/data/LSTM_full_2nd_export_2020_models_{k}.pkl', 'wb') as fi:\n",
    "                    pickle.dump(v, fi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neither of the 1st two models did particularly well. Spline was the better of the two. Next we will do a version without the log transform and see how that does in the spline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "08348bb306580ac2ff7a5930f57ccf23fdbb79f7c6aaa7428d52776827adbd16"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('other': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
